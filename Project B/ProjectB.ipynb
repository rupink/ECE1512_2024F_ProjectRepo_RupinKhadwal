{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "332eeea8-1daa-4f3b-a74a-d7e8deeb012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n",
      "Epoch 1, Loss: 0.06947039812803268, Time elapsed: 34.79 seconds\n",
      "Accuracy after Epoch 1: (95.88, 0.1297375554034409)%\n",
      "Epoch 2, Loss: 0.19341208040714264, Time elapsed: 47.14 seconds\n",
      "Epoch 3, Loss: 0.1235053539276123, Time elapsed: 44.46 seconds\n",
      "Epoch 4, Loss: 0.05402369052171707, Time elapsed: 59.55 seconds\n",
      "Epoch 5, Loss: 0.0843316912651062, Time elapsed: 45.93 seconds\n",
      "Epoch 6, Loss: 0.008823173120617867, Time elapsed: 42.91 seconds\n",
      "Accuracy after Epoch 6: (97.29, 0.06686426146988408)%\n",
      "Epoch 7, Loss: 0.05991634353995323, Time elapsed: 44.69 seconds\n",
      "Epoch 8, Loss: 0.01232814323157072, Time elapsed: 52.79 seconds\n",
      "Epoch 9, Loss: 0.016949331387877464, Time elapsed: 44.76 seconds\n",
      "Epoch 10, Loss: 0.09803974628448486, Time elapsed: 41.30 seconds\n",
      "Accuracy after Epoch 10: (97.57, 0.048369932942614616)%\n"
     ]
    }
   ],
   "source": [
    "# Part 3 & 4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import prune \n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_size, embed_size)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.multi_att(query, key, value)[0]\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embed_size=128, heads=8, num_layers=3, input_dim=784):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, embed_size)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_size, heads) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(embed_size, 10)  # MNIST has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        x = self.input_projection(x)  # Project input to the embedding size\n",
    "        query = key = value = x.unsqueeze(0)\n",
    "        for layer in self.layers:\n",
    "            x = layer(value, key, query)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Assume the rest of your code (data loading, training loop, etc.) is correct and follows this pattern.\n",
    "\n",
    "# Load MNIST data\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Pruning function to remove channels based on weight magnitude\n",
    "def prune_channels(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, epochs, device, optimizer, criterion, prune=False, start_pruning_after=5, pruning_increment=0.1):\n",
    "    total_pruning_rate = 0\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if prune and epoch >= start_pruning_after:\n",
    "            if total_pruning_rate < 0.5:  # Ensure not pruning more than 50%\n",
    "                prune_channels(model, amount=pruning_increment)\n",
    "                total_pruning_rate += pruning_increment\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Time elapsed: {elapsed_time:.2f} seconds')\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:  # Evaluate every 5 epochs and at the end\n",
    "            accuracy = evaluate_model(model, test_loader, device)\n",
    "            print(f'Accuracy after Epoch {epoch+1}: {accuracy}%')\n",
    "\n",
    "\n",
    "def entropy(predictions):\n",
    "    \"\"\"Calculates the entropy of the prediction probability distribution.\"\"\"\n",
    "    p_log_p = predictions * torch.log(predictions + 1e-9)  # Adding a small epsilon to prevent log(0)\n",
    "    return -p_log_p.sum(dim=1).mean()  # Sum over classes and average over the batch\n",
    "    \n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_entropy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            total_entropy += entropy(probabilities).item()  # Calculate entropy\n",
    "\n",
    "            pred = probabilities.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    average_entropy = total_entropy / len(test_loader)\n",
    "    return accuracy, average_entropy\n",
    "\n",
    "#def calculate_flops(model, input_size):\n",
    "#    inputs = torch.randn(1, *input_size)\n",
    "#    flop_analysis = FlopCountAnalysis(model, inputs)\n",
    "#    total_flops = flop_analysis.total()\n",
    "#    print(f\"Total FLOPs: {total_flops}\")\n",
    "#    return total_flops\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader = load_mnist_data()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train without pruning\n",
    "    print(\"Training baseline model...\")\n",
    "    train_model(model, train_loader, epochs=10, device=device, optimizer=optimizer, criterion=criterion)\n",
    "    baseline_accuracy = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # Reset and train with pruning\n",
    "    #model = TransformerModel().to(device)\n",
    "    #print(\"Training pruned model...\")\n",
    "    #train_model(model, train_loader, epochs=10, device=device, optimizer=optimizer, criterion=criterion, prune=True)\n",
    "    #pruned_accuracy = evaluate_model(model, test_loader, device)\n",
    "    #flops = calculate_flops(model, (1, 784))  # Assuming batch size of 1 for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7702fe88-7107-4dbb-b5c1-860d7d77cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with gated attention...\n",
      "Epoch 1, Loss: 0.04826178029179573, Time elapsed: 43.07 seconds\n",
      "Accuracy after Epoch 1: (95.98, 0.1126326176162345)%\n",
      "Epoch 2, Loss: 0.07568582892417908, Time elapsed: 48.80 seconds\n",
      "Epoch 3, Loss: 0.03277347981929779, Time elapsed: 47.38 seconds\n",
      "Epoch 4, Loss: 0.021255837753415108, Time elapsed: 73.80 seconds\n",
      "Epoch 5, Loss: 0.053934164345264435, Time elapsed: 75.29 seconds\n",
      "Epoch 6, Loss: 0.15232841670513153, Time elapsed: 44.05 seconds\n",
      "Accuracy after Epoch 6: (97.17, 0.061817129828428595)%\n",
      "Epoch 7, Loss: 0.04766477644443512, Time elapsed: 43.51 seconds\n",
      "Epoch 8, Loss: 0.09622547775506973, Time elapsed: 51.17 seconds\n",
      "Epoch 9, Loss: 0.11222010850906372, Time elapsed: 48.64 seconds\n",
      "Epoch 10, Loss: 0.1545521467924118, Time elapsed: 43.76 seconds\n",
      "Accuracy after Epoch 10: (96.94, 0.06794376086187773)%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define Transformer block with gated attention\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.gate = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_weights = torch.sigmoid(self.gate(x))\n",
    "        return x * gating_weights\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.gating = GatedAttention(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_size, embed_size)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        value = self.gating(value)\n",
    "        key = self.gating(key)\n",
    "        query = self.gating(query)\n",
    "        attention = self.multi_att(query, key, value)[0]\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embed_size=128, heads=8, num_layers=3, input_dim=784):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, embed_size)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_size, heads) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(embed_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.input_projection(x)\n",
    "        query = key = value = x.unsqueeze(0)\n",
    "        for layer in self.layers:\n",
    "            x = layer(value, key, query)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_entropy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            total_entropy += entropy(probabilities).item()  # Calculate entropy\n",
    "\n",
    "            pred = probabilities.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    average_entropy = total_entropy / len(test_loader)\n",
    "    return accuracy, average_entropy\n",
    "    \n",
    "def entropy(predictions):\n",
    "    \"\"\"Calculates the entropy of the prediction probability distribution.\"\"\"\n",
    "    p_log_p = predictions * torch.log(predictions + 1e-9)  # Adding a small epsilon to prevent log(0)\n",
    "    return -p_log_p.sum(dim=1).mean()  # Sum over classes and average over the batch\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, device, optimizer, criterion):\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Time elapsed: {elapsed_time:.2f} seconds')\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:  # Evaluate every 5 epochs and at the end\n",
    "            accuracy = evaluate_model(model, test_loader, device)\n",
    "            print(f'Accuracy after Epoch {epoch+1}: {accuracy}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader = load_mnist_data()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Training model with gated attention...\")\n",
    "    train_model(model, train_loader, test_loader, epochs=10, device=device, optimizer=optimizer, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9d719e-8833-4992-9260-a251263ce9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.1 / 1.2 SECOND\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn import LayerNorm\n",
    "import torch.optim as optim\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample.unsqueeze(0)  # Ensure there's a sequence dimension\n",
    "        return sample, self.labels[idx]\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna()  # Remove rows with NaN values\n",
    "\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "def prepare_loaders(X, y, batch_size=64, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    train_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "    test_data = torch.tensor(X_test, dtype=torch.float32)\n",
    "    train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "    test_labels = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    train_dataset = ECGDataset(train_data, train_labels)\n",
    "    test_dataset = ECGDataset(test_data, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class SelectiveSSM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelectiveSSM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.selective_gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.update_gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.state_transform = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.layer_norm = LayerNorm(hidden_dim)  # Layer normalization\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        #x_norm = self.layer_norm(x)  # Normalize inputs\n",
    "        \n",
    "        selectivity = torch.sigmoid(self.selective_gate(x))\n",
    "        updates = torch.tanh(self.update_gate(x))\n",
    "        #hidden = selectivity * updates + (1 - selectivity) * self.state_transform(hidden)\n",
    "\n",
    "        hidden_updated = selectivity * updates + (1 - selectivity) * self.state_transform(hidden)\n",
    "        hidden = self.layer_norm(hidden_updated)  # Normalize the updated hidden state\n",
    "        return hidden\n",
    "\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MambaModel, self).__init__()\n",
    "        self.ssm = SelectiveSSM(input_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(x.size(0), self.hidden_dim, device=x.device)\n",
    "        for t in range(x.size(1)):\n",
    "            hidden = self.ssm(x[:, t, :], hidden)\n",
    "        out = self.classifier(hidden)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MambaModel(input_dim=187, hidden_dim=256, output_dim=5)  # Corrected input_dim to match your dataset\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(train_loader, model, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Example usage\n",
    "file_path = 'C:/Users/rupin/Downloads/archive/mitbih_train.csv'\n",
    "X, y = load_data(file_path)\n",
    "train_loader, test_loader = prepare_loaders(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689ebc7f-f39b-409d-a20c-db6d6cc580a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.22339175641536713\n",
      "Epoch 2, Loss: 0.05657535418868065\n",
      "Epoch 3, Loss: 0.04412178695201874\n",
      "Epoch 4, Loss: 0.006150183733552694\n",
      "Epoch 5, Loss: 0.011675630696117878\n",
      "Epoch 6, Loss: 0.013105016201734543\n",
      "Epoch 7, Loss: 0.14350268244743347\n",
      "Epoch 8, Loss: 0.10360483080148697\n",
      "Epoch 9, Loss: 0.05790698900818825\n",
      "Epoch 10, Loss: 0.024175236001610756\n",
      "Accuracy on Test Set: 0.9790417451887385\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14577\n",
      "           1       0.89      0.69      0.78       418\n",
      "           2       0.94      0.93      0.94      1120\n",
      "           3       0.87      0.73      0.79       152\n",
      "           4       0.97      0.99      0.98      1244\n",
      "\n",
      "    accuracy                           0.98     17511\n",
      "   macro avg       0.93      0.87      0.90     17511\n",
      "weighted avg       0.98      0.98      0.98     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 1.2 \n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(test_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Inference mode, no gradients needed\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    return accuracy, report\n",
    "\n",
    "# Training the model\n",
    "train_model(train_loader, model, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy, report = evaluate_model(test_loader, model)\n",
    "print(f'Accuracy on Test Set: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437cee8e-0122-4d9d-a036-e8496ed4c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.008746728301048279\n",
      "Epoch 2, Loss: 0.012450456619262695\n",
      "Epoch 3, Loss: 0.025245103985071182\n",
      "Epoch 4, Loss: 0.10159114003181458\n",
      "Epoch 5, Loss: 0.0031929181423038244\n",
      "Epoch 6, Loss: 0.003471081145107746\n",
      "Epoch 7, Loss: 0.004381862469017506\n",
      "Epoch 8, Loss: 0.013778313994407654\n",
      "Epoch 9, Loss: 0.16024957597255707\n",
      "Epoch 10, Loss: 0.0021379387471824884\n",
      "Accuracy on Test Set: 0.9792701730340928\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14577\n",
      "           1       0.86      0.72      0.78       418\n",
      "           2       0.94      0.92      0.93      1120\n",
      "           3       0.86      0.71      0.78       152\n",
      "           4       0.99      0.98      0.99      1244\n",
      "\n",
      "    accuracy                           0.98     17511\n",
      "   macro avg       0.93      0.87      0.89     17511\n",
      "weighted avg       0.98      0.98      0.98     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SelectiveSSM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelectiveSSM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.selective_gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.update_gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.state_transform = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        #self.layer_norm = LayerNorm(hidden_dim)  # Layer normalization\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        selectivity = torch.sigmoid(self.selective_gate(x))\n",
    "        updates = torch.tanh(self.update_gate(x))\n",
    "        hidden = selectivity * updates + (1 - selectivity) * self.state_transform(hidden)\n",
    "        return hidden\n",
    "\n",
    "train_model(train_loader, model, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy, report = evaluate_model(test_loader, model)\n",
    "print(f'Accuracy on Test Set: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

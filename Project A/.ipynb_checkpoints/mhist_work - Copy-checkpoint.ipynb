{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e890593-a673-4c6d-8887-e6130e7d8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Acknowledgement to\n",
    "# https://github.com/kuangliu/pytorch-cifar,\n",
    "# https://github.com/BIGBALLON/CIFAR-ZOO,\n",
    "\n",
    "\n",
    "''' Swish activation '''\n",
    "class Swish(nn.Module): # Swish(x) = x∗σ(x)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "''' MLP '''\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n",
    "        self.fc_2 = nn.Linear(128, 128)\n",
    "        self.fc_3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0), -1)\n",
    "        out = F.relu(self.fc_1(out))\n",
    "        out = F.relu(self.fc_2(out))\n",
    "        out = self.fc_3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "''' ConvNet '''\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
    "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def embed(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "    def _get_activation(self, net_act):\n",
    "        if net_act == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif net_act == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif net_act == 'leakyrelu':\n",
    "            return nn.LeakyReLU(negative_slope=0.01)\n",
    "        elif net_act == 'swish':\n",
    "            return Swish()\n",
    "        else:\n",
    "            exit('unknown activation function: %s'%net_act)\n",
    "\n",
    "    def _get_pooling(self, net_pooling):\n",
    "        if net_pooling == 'maxpooling':\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'avgpooling':\n",
    "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_pooling: %s'%net_pooling)\n",
    "\n",
    "    def _get_normlayer(self, net_norm, shape_feat):\n",
    "        # shape_feat = (c*h*w)\n",
    "        if net_norm == 'batchnorm':\n",
    "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
    "        elif net_norm == 'layernorm':\n",
    "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
    "        elif net_norm == 'instancenorm':\n",
    "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
    "        elif net_norm == 'groupnorm':\n",
    "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
    "        elif net_norm == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_norm: %s'%net_norm)\n",
    "\n",
    "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
    "        layers = []\n",
    "        in_channels = channel\n",
    "        if im_size[0] == 28:\n",
    "            im_size = (32, 32)\n",
    "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
    "        for d in range(net_depth):\n",
    "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
    "            shape_feat[0] = net_width\n",
    "            if net_norm != 'none':\n",
    "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
    "            layers += [self._get_activation(net_act)]\n",
    "            in_channels = net_width\n",
    "            if net_pooling != 'none':\n",
    "                layers += [self._get_pooling(net_pooling)]\n",
    "                shape_feat[1] //= 2\n",
    "                shape_feat[2] //= 2\n",
    "\n",
    "        return nn.Sequential(*layers), shape_feat\n",
    "\n",
    "\n",
    "\n",
    "''' LeNet '''\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        self.fc_3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "''' AlexNet '''\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "''' AlexNetBN '''\n",
    "class AlexNetBN(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(AlexNetBN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "''' VGG '''\n",
    "cfg_vgg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n",
    "        super(VGG, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n",
    "        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def _make_layers(self, cfg, norm):\n",
    "        layers = []\n",
    "        in_channels = self.channel\n",
    "        for ic, x in enumerate(cfg):\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n",
    "                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def VGG11(channel, num_classes):\n",
    "    return VGG('VGG11', channel, num_classes)\n",
    "def VGG11BN(channel, num_classes):\n",
    "    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n",
    "def VGG13(channel, num_classes):\n",
    "    return VGG('VGG13', channel, num_classes)\n",
    "def VGG16(channel, num_classes):\n",
    "    return VGG('VGG16', channel, num_classes)\n",
    "def VGG19(channel, num_classes):\n",
    "    return VGG('VGG19', channel, num_classes)\n",
    "\n",
    "\n",
    "''' ResNet_AP '''\n",
    "# The conv(stride=2) is replaced by conv(stride=1) + avgpool(kernel_size=2, stride=2)\n",
    "\n",
    "class BasicBlock_AP(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(BasicBlock_AP, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2), # modification\n",
    "                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        if self.stride != 1: # modification\n",
    "            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck_AP(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(Bottleneck_AP, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),  # modification\n",
    "                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        if self.stride != 1: # modification\n",
    "            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_AP(nn.Module):\n",
    "    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n",
    "        super(ResNet_AP, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.norm = norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.classifier = nn.Linear(512 * block.expansion * 3 * 3 if channel==1 else 512 * block.expansion * 4 * 4, num_classes)  # modification\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.norm))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def embed(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "def ResNet18BN_AP(channel, num_classes):\n",
    "    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n",
    "\n",
    "def ResNet18_AP(channel, num_classes):\n",
    "    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes)\n",
    "\n",
    "\n",
    "''' ResNet '''\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.norm = norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.classifier = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.norm))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def embed(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18BN(channel, num_classes):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n",
    "\n",
    "def ResNet18(channel, num_classes):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)\n",
    "\n",
    "def ResNet34(channel, num_classes):\n",
    "    return ResNet(BasicBlock, [3,4,6,3], channel=channel, num_classes=num_classes)\n",
    "\n",
    "def ResNet50(channel, num_classes):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], channel=channel, num_classes=num_classes)\n",
    "\n",
    "def ResNet101(channel, num_classes):\n",
    "    return ResNet(Bottleneck, [3,4,23,3], channel=channel, num_classes=num_classes)\n",
    "\n",
    "def ResNet152(channel, num_classes):\n",
    "    return ResNet(Bottleneck, [3,8,36,3], channel=channel, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e0bda0-9569-4b07-a8bb-2dd63a48bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rupin\\AppData\\Local\\Temp\\ipykernel_3732\\435684228.py:9: DeprecationWarning: Please import `rotate` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
    "#from networks import MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n",
    "\n",
    "def get_dataset(dataset, data_path):\n",
    "    if dataset == 'MNIST':\n",
    "        channel = 1\n",
    "        im_size = (28, 28)\n",
    "        num_classes = 10\n",
    "        mean = [0.1307]\n",
    "        std = [0.3081]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = [str(c) for c in range(num_classes)]\n",
    "\n",
    "    elif dataset == 'FashionMNIST':\n",
    "        channel = 1\n",
    "        im_size = (28, 28)\n",
    "        num_classes = 10\n",
    "        mean = [0.2861]\n",
    "        std = [0.3530]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'SVHN':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 10\n",
    "        mean = [0.4377, 0.4438, 0.4728]\n",
    "        std = [0.1980, 0.2010, 0.1970]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n",
    "        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n",
    "        class_names = [str(c) for c in range(num_classes)]\n",
    "\n",
    "    elif dataset == 'CIFAR10':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 10\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'CIFAR100':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 100\n",
    "        mean = [0.5071, 0.4866, 0.4409]\n",
    "        std = [0.2673, 0.2564, 0.2762]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'TinyImageNet':\n",
    "        channel = 3\n",
    "        im_size = (64, 64)\n",
    "        num_classes = 200\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n",
    "\n",
    "        class_names = data['classes']\n",
    "\n",
    "        images_train = data['images_train']\n",
    "        labels_train = data['labels_train']\n",
    "        images_train = images_train.detach().float() / 255.0\n",
    "        labels_train = labels_train.detach()\n",
    "        for c in range(channel):\n",
    "            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n",
    "        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n",
    "\n",
    "        images_val = data['images_val']\n",
    "        labels_val = data['labels_val']\n",
    "        images_val = images_val.detach().float() / 255.0\n",
    "        labels_val = labels_val.detach()\n",
    "\n",
    "        for c in range(channel):\n",
    "            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n",
    "\n",
    "        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n",
    "\n",
    "    else:\n",
    "        exit('unknown dataset: %s'%dataset)\n",
    "\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=0)\n",
    "    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader\n",
    "\n",
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
    "        self.images = images.detach().float()\n",
    "        self.labels = labels.detach()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "def get_default_convnet_setting():\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
    "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
    "\n",
    "\n",
    "\n",
    "def get_network(model, channel, num_classes, im_size=(32, 32)):\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
    "\n",
    "    if model == 'MLP':\n",
    "        net = MLP(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'ConvNet':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'LeNet':\n",
    "        net = LeNet(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'AlexNet':\n",
    "        net = AlexNet(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'AlexNetBN':\n",
    "        net = AlexNetBN(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'VGG11':\n",
    "        net = VGG11( channel=channel, num_classes=num_classes)\n",
    "    elif model == 'VGG11BN':\n",
    "        net = VGG11BN(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'ResNet18':\n",
    "        net = ResNet18(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'ResNet18BN_AP':\n",
    "        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'ResNet18BN':\n",
    "        net = ResNet18BN(channel=channel, num_classes=num_classes)\n",
    "\n",
    "    elif model == 'ConvNetD1':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD2':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD3':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD4':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD7':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetW32':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW64':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW128':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW256':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetAS':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetAR':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetAL':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetASwish':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetASwishBN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetNN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetBN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetLN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetIN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetGN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetNP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n",
    "    elif model == 'ConvNetMP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n",
    "    elif model == 'ConvNetAP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n",
    "\n",
    "    else:\n",
    "        net = None\n",
    "        exit('unknown model: %s'%model)\n",
    "\n",
    "    gpu_num = torch.cuda.device_count()\n",
    "    if gpu_num>0:\n",
    "        device = 'cuda'\n",
    "        if gpu_num>1:\n",
    "            net = nn.DataParallel(net)\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    net = net.to(device)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "\n",
    "def distance_wb(gwr, gws):\n",
    "    shape = gwr.shape\n",
    "    if len(shape) == 4: # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2: # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n",
    "\n",
    "    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n",
    "    dis = dis_weight\n",
    "    return dis\n",
    "\n",
    "\n",
    "\n",
    "def match_loss(gw_syn, gw_real, args):\n",
    "    dis = torch.tensor(0.0).to(args.device)\n",
    "\n",
    "    if args.dis_metric == 'ours':\n",
    "        for ig in range(len(gw_real)):\n",
    "            gwr = gw_real[ig]\n",
    "            gws = gw_syn[ig]\n",
    "            dis += distance_wb(gwr, gws)\n",
    "\n",
    "    elif args.dis_metric == 'mse':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n",
    "\n",
    "    elif args.dis_metric == 'cos':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n",
    "\n",
    "    else:\n",
    "        exit('unknown distance function: %s'%args.dis_metric)\n",
    "\n",
    "    return dis\n",
    "\n",
    "\n",
    "\n",
    "def get_loops(ipc):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if ipc == 1:\n",
    "        outer_loop, inner_loop = 1, 1\n",
    "    elif ipc == 10:\n",
    "        outer_loop, inner_loop = 10, 50\n",
    "    elif ipc == 20:\n",
    "        outer_loop, inner_loop = 20, 25\n",
    "    elif ipc == 30:\n",
    "        outer_loop, inner_loop = 30, 20\n",
    "    elif ipc == 40:\n",
    "        outer_loop, inner_loop = 40, 15\n",
    "    elif ipc == 50:\n",
    "        outer_loop, inner_loop = 50, 10\n",
    "    else:\n",
    "        outer_loop, inner_loop = 0, 0\n",
    "        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n",
    "    return outer_loop, inner_loop\n",
    "\n",
    "\n",
    "\n",
    "def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "    net = net.to(args.device)\n",
    "    criterion = criterion.to(args.device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        net.train()\n",
    "    else:\n",
    "        net.eval()\n",
    "\n",
    "    for i_batch, datum in enumerate(dataloader):\n",
    "        \n",
    "        img = datum[0].float().to(args.device)\n",
    "        #print(datum, img)\n",
    "        #datum[0] = torch.tensor(datum[0])\n",
    "        if aug:\n",
    "            if args.dsa:\n",
    "                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n",
    "            else:\n",
    "                img = augment(img, args.dc_aug_param, device=args.device)\n",
    "        # Ensure `lab` is 1D by adding an extra dimension if it's a scalar\n",
    "        lab = datum[1].to(args.device).long()\n",
    "        if lab.ndim == 0:  # If lab is a scalar\n",
    "            lab = lab.unsqueeze(0)\n",
    "        #print(lab)\n",
    "        #lab = lab.unsqueeze(0)\n",
    "        if img.ndim == 3:  # If shape is [128, 32,S 32]\n",
    "            img = img.unsqueeze(0)\n",
    "        \n",
    "\n",
    "        n_b = lab.shape[0]\n",
    "\n",
    "        output = net(img)\n",
    "        loss = criterion(output, lab)\n",
    "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += acc\n",
    "        num_exp += n_b\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    return loss_avg, acc_avg\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n",
    "    net = net.to(args.device)\n",
    "    images_train = images_train.to(args.device)\n",
    "    labels_train = labels_train.to(args.device)\n",
    "    lr = float(args.lr_net)\n",
    "    Epoch = int(args.epoch_eval_train)\n",
    "    lr_schedule = [Epoch//2+1]\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "\n",
    "    dst_train = TensorDataset(images_train, labels_train)\n",
    "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
    "\n",
    "    start = time.time()\n",
    "    for ep in range(Epoch+1):\n",
    "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug = True)\n",
    "        if ep in lr_schedule:\n",
    "            lr *= 0.1\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    time_train = time.time() - start\n",
    "    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug = False)\n",
    "    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n",
    "\n",
    "    return net, acc_train, acc_test\n",
    "\n",
    "\n",
    "\n",
    "def augment(images, dc_aug_param, device):\n",
    "    # This can be sped up in the future.\n",
    "\n",
    "    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n",
    "        scale = dc_aug_param['scale']\n",
    "        crop = dc_aug_param['crop']\n",
    "        rotate = dc_aug_param['rotate']\n",
    "        noise = dc_aug_param['noise']\n",
    "        strategy = dc_aug_param['strategy']\n",
    "\n",
    "        shape = images.shape\n",
    "        mean = []\n",
    "        for c in range(shape[1]):\n",
    "            mean.append(float(torch.mean(images[:,c])))\n",
    "\n",
    "        def cropfun(i):\n",
    "            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n",
    "            for c in range(shape[1]):\n",
    "                im_[c] = mean[c]\n",
    "            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n",
    "            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n",
    "            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n",
    "\n",
    "        def scalefun(i):\n",
    "            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
    "            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
    "            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n",
    "            mhw = max(h, w, shape[2], shape[3])\n",
    "            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n",
    "            r = int((mhw - h) / 2)\n",
    "            c = int((mhw - w) / 2)\n",
    "            im_[:, r:r + h, c:c + w] = tmp\n",
    "            r = int((mhw - shape[2]) / 2)\n",
    "            c = int((mhw - shape[3]) / 2)\n",
    "            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n",
    "\n",
    "        def rotatefun(i):\n",
    "            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n",
    "            r = int((im_.shape[-2] - shape[-2]) / 2)\n",
    "            c = int((im_.shape[-1] - shape[-1]) / 2)\n",
    "            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n",
    "\n",
    "        def noisefun(i):\n",
    "            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "        augs = strategy.split('_')\n",
    "\n",
    "        for i in range(shape[0]):\n",
    "            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n",
    "            if choice == 'crop':\n",
    "                cropfun(i)\n",
    "            elif choice == 'scale':\n",
    "                scalefun(i)\n",
    "            elif choice == 'rotate':\n",
    "                rotatefun(i)\n",
    "            elif choice == 'noise':\n",
    "                noisefun(i)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def get_daparam(dataset, model, model_eval, ipc):\n",
    "    # We find that augmentation doesn't always benefit the performance.\n",
    "    # So we do augmentation for some of the settings.\n",
    "\n",
    "    dc_aug_param = dict()\n",
    "    dc_aug_param['crop'] = 4\n",
    "    dc_aug_param['scale'] = 0.2\n",
    "    dc_aug_param['rotate'] = 45\n",
    "    dc_aug_param['noise'] = 0.001\n",
    "    dc_aug_param['strategy'] = 'none'\n",
    "\n",
    "    if dataset == 'MNIST':\n",
    "        dc_aug_param['strategy'] = 'crop_scale_rotate'\n",
    "\n",
    "    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n",
    "        dc_aug_param['strategy'] = 'crop_noise'\n",
    "\n",
    "    return dc_aug_param\n",
    "\n",
    "\n",
    "def get_eval_pool(eval_mode, model, model_eval):\n",
    "    if eval_mode == 'M': # multiple architectures\n",
    "        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n",
    "    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n",
    "        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n",
    "    elif eval_mode == 'W': # ablation study on network width\n",
    "        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n",
    "    elif eval_mode == 'D': # ablation study on network depth\n",
    "        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n",
    "    elif eval_mode == 'A': # ablation study on network activation function\n",
    "        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n",
    "    elif eval_mode == 'P': # ablation study on network pooling layer\n",
    "        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n",
    "    elif eval_mode == 'N': # ablation study on network normalization layer\n",
    "        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n",
    "    elif eval_mode == 'S': # itself\n",
    "        if 'BN' in model:\n",
    "            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n",
    "        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n",
    "    elif eval_mode == 'SS':  # itself\n",
    "        model_eval_pool = [model]\n",
    "    else:\n",
    "        model_eval_pool = [model_eval]\n",
    "    return model_eval_pool\n",
    "\n",
    "\n",
    "class ParamDiffAug():\n",
    "    def __init__(self):\n",
    "        self.aug_mode = 'S' #'multiple or single'\n",
    "        self.prob_flip = 0.5\n",
    "        self.ratio_scale = 1.2\n",
    "        self.ratio_rotate = 15.0\n",
    "        self.ratio_crop_pad = 0.125\n",
    "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
    "        self.brightness = 1.0\n",
    "        self.saturation = 2.0\n",
    "        self.contrast = 0.5\n",
    "\n",
    "\n",
    "def set_seed_DiffAug(param):\n",
    "    if param.latestseed == -1:\n",
    "        return\n",
    "    else:\n",
    "        torch.random.manual_seed(param.latestseed)\n",
    "        param.latestseed += 1\n",
    "\n",
    "\n",
    "def DiffAugment(x, strategy='', seed = -1, param = None):\n",
    "    if strategy == 'None' or strategy == 'none' or strategy == '':\n",
    "        return x\n",
    "\n",
    "    if seed == -1:\n",
    "        param.Siamese = False\n",
    "    else:\n",
    "        param.Siamese = True\n",
    "\n",
    "    param.latestseed = seed\n",
    "\n",
    "    if strategy:\n",
    "        if param.aug_mode == 'M': # original\n",
    "            for p in strategy.split('_'):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, param)\n",
    "        elif param.aug_mode == 'S':\n",
    "            pbties = strategy.split('_')\n",
    "            set_seed_DiffAug(param)\n",
    "            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x, param)\n",
    "        else:\n",
    "            exit('unknown augmentation mode: %s'%param.aug_mode)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n",
    "def rand_scale(x, param):\n",
    "    # x>1, max scale\n",
    "    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n",
    "    ratio = param.ratio_scale\n",
    "    set_seed_DiffAug(param)\n",
    "    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
    "    set_seed_DiffAug(param)\n",
    "    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
    "    theta = [[[sx[i], 0,  0],\n",
    "            [0,  sy[i], 0],] for i in range(x.shape[0])]\n",
    "    theta = torch.tensor(theta, dtype=torch.float)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        theta[:] = theta[0]\n",
    "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n",
    "    ratio = param.ratio_rotate\n",
    "    set_seed_DiffAug(param)\n",
    "    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n",
    "    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n",
    "        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n",
    "    theta = torch.tensor(theta, dtype=torch.float)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        theta[:] = theta[0]\n",
    "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_flip(x, param):\n",
    "    prob = param.prob_flip\n",
    "    set_seed_DiffAug(param)\n",
    "    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        randf[:] = randf[0]\n",
    "    return torch.where(randf < prob, x.flip(3), x)\n",
    "\n",
    "\n",
    "def rand_brightness(x, param):\n",
    "    ratio = param.brightness\n",
    "    set_seed_DiffAug(param)\n",
    "    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        randb[:] = randb[0]\n",
    "    x = x + (randb - 0.5)*ratio\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x, param):\n",
    "    ratio = param.saturation\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    set_seed_DiffAug(param)\n",
    "    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        rands[:] = rands[0]\n",
    "    x = (x - x_mean) * (rands * ratio) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x, param):\n",
    "    ratio = param.contrast\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    set_seed_DiffAug(param)\n",
    "    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        randc[:] = randc[0]\n",
    "    x = (x - x_mean) * (randc + ratio) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_crop(x, param):\n",
    "    # The image is padded on its surrounding and then cropped.\n",
    "    ratio = param.ratio_crop_pad\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    set_seed_DiffAug(param)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    set_seed_DiffAug(param)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        translation_x[:] = translation_x[0]\n",
    "        translation_y[:] = translation_y[0]\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, param):\n",
    "    ratio = param.ratio_cutout\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    set_seed_DiffAug(param)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    set_seed_DiffAug(param)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        offset_x[:] = offset_x[0]\n",
    "        offset_y[:] = offset_y[0]\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'crop': [rand_crop],\n",
    "    'cutout': [rand_cutout],\n",
    "    'flip': [rand_flip],\n",
    "    'scale': [rand_scale],\n",
    "    'rotate': [rand_rotate],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "482f5d28-a701-44fa-b334-9dff24b4eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MHIST dataset loader\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Define the MHIST dataset class\n",
    "class MHISTDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (str): Path to the csv file with annotations.\n",
    "            img_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.label_mapping = {'SSA': 0, 'HP': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        # Convert label from string to integer using mapping\n",
    "        label_str = self.img_labels.iloc[idx, 1]\n",
    "        label = self.label_mapping[label_str]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define the transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Path to MHIST images and annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82441a1c-5343-44d1-848e-b5a367eed0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Define the MHIST dataset class\n",
    "class MHISTDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, partition='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (str): Path to the CSV file with annotations.\n",
    "            img_dir (str): Directory with all the images.\n",
    "            partition (str): Either 'train' or 'test' to select the partition.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.partition = partition\n",
    "\n",
    "        # Filter data based on the specified partition\n",
    "        self.img_labels = self.img_labels[self.img_labels['Partition'] == partition].reset_index(drop=True)\n",
    "        \n",
    "        # Define label mapping for 'SSA' and 'HP'\n",
    "        self.label_mapping = {'SSA': 0, 'HP': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name and label\n",
    "        img_name = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        # Convert label from string to integer using mapping\n",
    "        label_str = self.img_labels.iloc[idx, 1]\n",
    "        label = self.label_mapping[label_str]\n",
    "        \n",
    "        # Apply the provided transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define the transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "# Paths to MHIST images and annotations\n",
    "img_dir = \"C:/Users/rupin/Downloads/ECE1512_2024F_ProjectA_submission_files/submission_files/mhist_dataset/images/images\"\n",
    "annotations_file = r\"C:\\Users\\rupin\\Downloads\\ECE1512_2024F_ProjectA_submission_files\\submission_files\\mhist_dataset\\annotations.csv\"\n",
    "\n",
    "# Load the train and test partitions\n",
    "train_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, partition='train', transform=transform)\n",
    "test_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, partition='test', transform=transform)\n",
    "\n",
    "# Example DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "860030e0-0677-4ec2-8e65-e162e0bf9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Args Params\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device ='cpu'\n",
    "        self.lr_net = 0.01  # learning rate for evaluation\n",
    "        self.epoch_eval_train = 20  # epochs for training during evaluation\n",
    "        self.batch_train = 128\n",
    "        self.dsa = False\n",
    "        self.dsa_strategy = ''  # leave blank if DiffAugment is not used\n",
    "        self.dc_aug_param = None  # specify if using `augment` function with defined parameters\n",
    "        self.dis_metric = 'cos'\n",
    "\n",
    "# Instantiate args\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf966c12-f7a3-4afd-86e7-50c9198c13ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Iteration 1/200, SAM Loss: 1.1616684198379517\n",
      "Iteration 2/200, SAM Loss: 0.5336136221885681\n",
      "Iteration 3/200, SAM Loss: 0.19349677860736847\n",
      "Iteration 4/200, SAM Loss: 1.0844089984893799\n",
      "Iteration 5/200, SAM Loss: 1.229508876800537\n",
      "Iteration 6/200, SAM Loss: 1.7224167585372925\n",
      "Iteration 7/200, SAM Loss: 0.15170986950397491\n",
      "Iteration 8/200, SAM Loss: 0.05776948481798172\n",
      "Iteration 9/200, SAM Loss: 1.853320837020874\n",
      "Iteration 10/200, SAM Loss: 0.052461571991443634\n",
      "Iteration 11/200, SAM Loss: 1.54939866065979\n",
      "Iteration 12/200, SAM Loss: 1.3858089447021484\n",
      "Iteration 13/200, SAM Loss: 0.05423266440629959\n",
      "Iteration 14/200, SAM Loss: 0.8164035081863403\n",
      "Iteration 15/200, SAM Loss: 0.11931497603654861\n",
      "Iteration 16/200, SAM Loss: 0.1263720542192459\n",
      "Iteration 17/200, SAM Loss: 0.20093996822834015\n",
      "Iteration 18/200, SAM Loss: 0.6777393817901611\n",
      "Iteration 19/200, SAM Loss: 0.19717450439929962\n",
      "Iteration 20/200, SAM Loss: 0.08135402947664261\n",
      "Iteration 21/200, SAM Loss: 0.9265415668487549\n",
      "Iteration 22/200, SAM Loss: 0.7735027074813843\n",
      "Iteration 23/200, SAM Loss: 0.21490992605686188\n",
      "Iteration 24/200, SAM Loss: 0.7262306213378906\n",
      "Iteration 25/200, SAM Loss: 0.07870269566774368\n",
      "Iteration 26/200, SAM Loss: 0.16889049112796783\n",
      "Iteration 27/200, SAM Loss: 1.3613545894622803\n",
      "Iteration 28/200, SAM Loss: 0.9982631802558899\n",
      "Iteration 29/200, SAM Loss: 1.1098500490188599\n",
      "Iteration 30/200, SAM Loss: 0.3510371446609497\n",
      "Iteration 31/200, SAM Loss: 0.40376198291778564\n",
      "Iteration 32/200, SAM Loss: 1.2090173959732056\n",
      "Iteration 33/200, SAM Loss: 0.1022692397236824\n",
      "Iteration 34/200, SAM Loss: 0.3964948058128357\n",
      "Iteration 35/200, SAM Loss: 0.1971169263124466\n",
      "Iteration 36/200, SAM Loss: 0.2981404662132263\n",
      "Iteration 37/200, SAM Loss: 0.1940394788980484\n",
      "Iteration 38/200, SAM Loss: 1.117541790008545\n",
      "Iteration 39/200, SAM Loss: 0.12525589764118195\n",
      "Iteration 40/200, SAM Loss: 0.3431844711303711\n",
      "Iteration 41/200, SAM Loss: 0.9564976096153259\n",
      "Iteration 42/200, SAM Loss: 1.0906373262405396\n",
      "Iteration 43/200, SAM Loss: 0.24916298687458038\n",
      "Iteration 44/200, SAM Loss: 0.16317661106586456\n",
      "Iteration 45/200, SAM Loss: 1.6131322383880615\n",
      "Iteration 46/200, SAM Loss: 1.5727587938308716\n",
      "Iteration 47/200, SAM Loss: 0.3585166931152344\n",
      "Iteration 48/200, SAM Loss: 1.363072395324707\n",
      "Iteration 49/200, SAM Loss: 0.8534024357795715\n",
      "Iteration 50/200, SAM Loss: 0.23359479010105133\n",
      "Iteration 51/200, SAM Loss: 0.3846830725669861\n",
      "Iteration 52/200, SAM Loss: 0.36891472339630127\n",
      "Iteration 53/200, SAM Loss: 0.1203358843922615\n",
      "Iteration 54/200, SAM Loss: 0.13229377567768097\n",
      "Iteration 55/200, SAM Loss: 0.5520249009132385\n",
      "Iteration 56/200, SAM Loss: 0.4350441098213196\n",
      "Iteration 57/200, SAM Loss: 0.041625089943408966\n",
      "Iteration 58/200, SAM Loss: 0.7133409976959229\n",
      "Iteration 59/200, SAM Loss: 0.8429176211357117\n",
      "Iteration 60/200, SAM Loss: 0.3165212869644165\n",
      "Iteration 61/200, SAM Loss: 0.595492959022522\n",
      "Iteration 62/200, SAM Loss: 0.11249852925539017\n",
      "Iteration 63/200, SAM Loss: 1.3666045665740967\n",
      "Iteration 64/200, SAM Loss: 0.36474472284317017\n",
      "Iteration 65/200, SAM Loss: 0.20729269087314606\n",
      "Iteration 66/200, SAM Loss: 0.21957774460315704\n",
      "Iteration 67/200, SAM Loss: 0.2779551148414612\n",
      "Iteration 68/200, SAM Loss: 0.22791112959384918\n",
      "Iteration 69/200, SAM Loss: 0.27105629444122314\n",
      "Iteration 70/200, SAM Loss: 0.28472787141799927\n",
      "Iteration 71/200, SAM Loss: 1.7362356185913086\n",
      "Iteration 72/200, SAM Loss: 0.9109903573989868\n",
      "Iteration 73/200, SAM Loss: 1.0667197704315186\n"
     ]
    }
   ],
   "source": [
    "#Part b\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# MHIST dataset directory and annotations file\n",
    "img_dir = \"C:/Users/rupin/Downloads/ECE1512_2024F_ProjectA_submission_files/submission_files/mhist_dataset/images/images\"\n",
    "annotations_file = r\"C:\\Users\\rupin\\Downloads\\ECE1512_2024F_ProjectA_submission_files\\submission_files\\mhist_dataset\\annotations.csv\"\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (e.g., 224x224 for ConvNet)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Initialize the MHIST dataset and DataLoader\n",
    "mhist_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=transform)\n",
    "batch_size = 128\n",
    "real_data_loader = DataLoader(mhist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the synthetic dataset with Gaussian noise (mean=0, std=1)\n",
    "num_classes = 2  # For SSA and HP labels\n",
    "channel = 3\n",
    "im_size = (224, 224)  # Resize all images to 224x224\n",
    "synthetic_data = torch.randn(num_classes, channel, im_size[0], im_size[1], requires_grad=True, device=device)\n",
    "\n",
    "# Set up the model (ConvNet-7 for MHIST) and optimizer\n",
    "model_name = 'ConvNetD3'  # Use ConvNet-7 for MHIST dataset\n",
    "model = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "\n",
    "# SAM Hyperparameters\n",
    "K = 200  # Number of random weight initializations\n",
    "T = 10   # Number of iterations per weight initialization\n",
    "eta_S = 0.1  # Learning rate for synthetic samples\n",
    "lambda_param = 0.01  # Task balance parameter\n",
    "\n",
    "# Optimizer for synthetic data\n",
    "optimizer_syn = optim.SGD([synthetic_data], lr=eta_S)\n",
    "\n",
    "# Define args for match_loss (if needed)\n",
    "class Args:\n",
    "    def __init__(self, device, dis_metric='cos'):\n",
    "        self.device = device\n",
    "        self.dis_metric = dis_metric\n",
    "\n",
    "args = Args(device=device)  # Instantiate args for match_loss\n",
    "\n",
    "# SAM Training loop for MHIST with Gaussian-initialized synthetic data\n",
    "for k in range(K):\n",
    "    # Randomly initialize the model parameters\n",
    "    model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "    optimizer_model = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for t in range(T):\n",
    "        optimizer_syn.zero_grad()\n",
    "        optimizer_model.zero_grad()\n",
    "\n",
    "        # Fetch a batch of real data from DataLoader\n",
    "        real_images, real_labels = next(iter(real_data_loader))\n",
    "        real_images, real_labels = real_images.to(device), real_labels.to(device)\n",
    "\n",
    "        # Select a subset of 2 samples from real data to match synthetic data size\n",
    "        indices = torch.randperm(real_images.size(0))[:2]\n",
    "        real_images_subset = real_images[indices]\n",
    "        real_labels_subset = real_labels[indices]\n",
    "\n",
    "        # Forward pass on both real data and synthetic data\n",
    "        outputs_real = model(real_images_subset)  # Real data outputs (2 samples)\n",
    "        outputs_syn = model(synthetic_data)       # Synthetic data outputs (2 samples)\n",
    "\n",
    "        # Calculate the SAM loss (Attention Matching) and optimize synthetic data\n",
    "        loss = match_loss(outputs_syn, outputs_real, args) + lambda_param\n",
    "        loss.backward()\n",
    "        optimizer_syn.step()  # Update synthetic data based on SAM loss\n",
    "\n",
    "    print(f\"Iteration {k + 1}/{K}, SAM Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ef9cc-2ab6-4297-b0b4-84667be903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Ensure device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Visualization Function for Synthetic Data\n",
    "def visualize_synthetic_data(synthetic_data, class_names):\n",
    "    synthetic_data = synthetic_data.detach().cpu()\n",
    "    fig, axes = plt.subplots(1, len(class_names), figsize=(10, 5))\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(synthetic_data[i].permute(1, 2, 0))  # Convert from CHW to HWC for display\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Class {class_names[i]}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Part (c): Visualize the synthetic images\n",
    "class_names = ['SSA', 'HP']\n",
    "print(\"Visualizing synthetic images after Part (b):\")\n",
    "visualize_synthetic_data(synthetic_data, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3056c4-1ebe-4440-835e-2220c71c7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "#from utils import match_loss, get_network\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths for the MHIST dataset\n",
    "img_dir = r\"C:\\Users\\rupin\\Downloads\\ECE1512_2024F_ProjectA_submission_files\\submission_files\\mhist_dataset\\images\\images\"\n",
    "annotations_file = r\"C:\\Users\\rupin\\Downloads\\ECE1512_2024F_ProjectA_submission_files\\submission_files\\mhist_dataset\\annotations.csv\"\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust as needed for ConvNet-7\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Initialize MHIST dataset and DataLoader for real data\n",
    "mhist_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=transform)\n",
    "batch_size = 256\n",
    "real_data_loader = DataLoader(mhist_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Reinitialize synthetic data with Gaussian noise\n",
    "num_classes = 2  # Classes: SSA and HP\n",
    "channel = 3\n",
    "im_size = (224, 224)  # Image size used in transform\n",
    "# Part (d) - Reinitialize synthetic data with Gaussian noise (mean=0.5, std=0.1, for example)\n",
    "gaussian_mean = 0.5\n",
    "gaussian_std = 0.1\n",
    "synthetic_data = (gaussian_std * torch.randn(num_classes, channel, im_size[0], im_size[1], device=device) + gaussian_mean).requires_grad_(True)\n",
    "\n",
    "\n",
    "# Set up the model (ConvNet-7 for MHIST) and optimizer\n",
    "model_name = 'ConvNetD3'  # ConvNet-7 for MHIST dataset\n",
    "model = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "\n",
    "# SAM Hyperparameters\n",
    "K = 100  # Number of random weight initializations\n",
    "T = 10   # Number of iterations per weight initialization\n",
    "eta_S = 0.1  # Learning rate for synthetic samples\n",
    "lambda_param = 0.01  # Task balance parameter\n",
    "\n",
    "# Optimizer for synthetic data\n",
    "optimizer_syn = optim.SGD([synthetic_data], lr=eta_S)\n",
    "\n",
    "# SAM Training loop with Gaussian-initialized synthetic data\n",
    "for k in range(K):\n",
    "    # Randomly initialize the model parameters\n",
    "    model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "    optimizer_model = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for t in range(T):\n",
    "        optimizer_syn.zero_grad()\n",
    "        optimizer_model.zero_grad()\n",
    "\n",
    "        # Fetch a batch of real data from DataLoader\n",
    "        real_images, real_labels = next(iter(real_data_loader))\n",
    "        real_images, real_labels = real_images.to(device), real_labels.to(device)\n",
    "\n",
    "        # Select a subset of 2 samples from real data to match synthetic data size\n",
    "        indices = torch.randperm(real_images.size(0))[:2]\n",
    "        real_images_subset = real_images[indices]\n",
    "        real_labels_subset = real_labels[indices]\n",
    "\n",
    "        # Forward pass on both real data and synthetic data\n",
    "        outputs_real = model(real_images_subset)  # Real data outputs (2 samples)\n",
    "        outputs_syn = model(synthetic_data)       # Synthetic data outputs (2 samples)\n",
    "\n",
    "        # Calculate the SAM loss (Attention Matching) and optimize synthetic data\n",
    "        loss = match_loss(outputs_syn, outputs_real, args) + lambda_param\n",
    "        loss.backward()\n",
    "        optimizer_syn.step()  # Update synthetic data based on SAM loss\n",
    "\n",
    "    print(f\"Iteration {k + 1}/{K}, SAM Loss: {loss.item()}\")\n",
    "\n",
    "# Visualization of SAM Results for Synthetic Data after Gaussian Initialization\n",
    "def visualize_sam_results(synthetic_data, class_names):\n",
    "    \"\"\"\n",
    "    Visualizes the synthetic images generated for each class using SAM.\n",
    "    \n",
    "    Args:\n",
    "        synthetic_data (Tensor): Synthetic images tensor with shape [num_classes, channels, height, width].\n",
    "        class_names (list): List of class names, e.g., ['SSA', 'HP'].\n",
    "    \"\"\"\n",
    "    synthetic_data = synthetic_data.detach().cpu()  # Move data to CPU and detach from computation graph\n",
    "    num_classes = synthetic_data.size(0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_classes, figsize=(10, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Convert from CHW to HWC for display\n",
    "        ax.imshow(synthetic_data[i].permute(1, 2, 0))  # CHW to HWC\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Class {class_names[i]}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Define class names for MHIST\n",
    "class_names = ['SSA', 'HP']\n",
    "\n",
    "# Visualize synthetic data results after SAM training with Gaussian initialization\n",
    "print(\"Visualizing SAM results for synthetic images after Gaussian initialization:\")\n",
    "visualize_sam_results(synthetic_data, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5b335-e6dd-4ac2-89b4-7857b116d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define train-test split ratio (e.g., 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(mhist_dataset))\n",
    "test_size = len(mhist_dataset) - train_size\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(mhist_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders for training and testing on real data\n",
    "batch_size = 64  # Define an appropriate batch size\n",
    "real_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define function for model evaluation\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Define hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# PART 1: Train on Synthetic Data\n",
    "print(\"Training on Synthetic Data...\")\n",
    "model_name = 'ConvNetD3'  # Example model name, replace with appropriate model\n",
    "channel = 3  # Number of channels, adjust if necessary\n",
    "num_classes = 2  # Number of classes, e.g., SSA and HP\n",
    "im_size = (224, 224)  # Image size\n",
    "\n",
    "# Initialize the model and optimizer for synthetic data\n",
    "model_synthetic = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "optimizer_synthetic = optim.SGD(model_synthetic.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a DataLoader for the synthetic dataset with batch size 1\n",
    "synthetic_loader = DataLoader(list(zip(synthetic_data, torch.arange(num_classes))), batch_size=1, shuffle=True)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    model_synthetic.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in synthetic_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_synthetic.zero_grad()\n",
    "        outputs = model_synthetic(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_synthetic.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100.0 * total_correct / len(synthetic_loader)\n",
    "    print(f\"[Synthetic Data] Epoch {epoch_num + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate model trained on synthetic data\n",
    "accuracy_synthetic = evaluate_model(model_synthetic, test_loader, device)\n",
    "print(f\"Test Accuracy of model trained on synthetic data: {accuracy_synthetic:.2f}%\")\n",
    "\n",
    "# PART 2: Train on Real Data\n",
    "print(\"Training on Real Data...\")\n",
    "model_real = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "optimizer_real = optim.SGD(model_real.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    model_real.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in real_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_real.zero_grad()\n",
    "        outputs = model_real(images)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_real.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100.0 * total_correct / len(real_data_loader)\n",
    "    print(f\"[Real Data] Epoch {epoch_num + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate model trained on real data\n",
    "accuracy_real = evaluate_model(model_real, test_loader, device)\n",
    "print(f\"Test Accuracy of model trained on real data: {accuracy_real:.2f}%\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(f\"Model trained on synthetic data - Test Accuracy: {accuracy_synthetic:.2f}%\")\n",
    "print(f\"Model trained on real data - Test Accuracy: {accuracy_real:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfb138-5acb-4aee-aed5-2681d04774cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part e\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define train-test split ratio (e.g., 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(mhist_dataset))\n",
    "test_size = len(mhist_dataset) - train_size\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(mhist_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders for training and testing on real data\n",
    "batch_size = 64  # Define an appropriate batch size\n",
    "real_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define function for model evaluation\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Define hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# PART 1: Train on Synthetic Data\n",
    "print(\"Training on Synthetic Data...\")\n",
    "model_name = 'ConvNetD3'  # Example model name, replace with appropriate model\n",
    "channel = 3  # Number of channels, adjust if necessary\n",
    "num_classes = 2  # Number of classes, e.g., SSA and HP\n",
    "im_size = (224, 224)  # Image size\n",
    "\n",
    "# Initialize the model and optimizer for synthetic data\n",
    "model_synthetic = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "optimizer_synthetic = optim.SGD(model_synthetic.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a DataLoader for the synthetic dataset with batch size 1\n",
    "synthetic_loader = DataLoader(list(zip(synthetic_data, torch.arange(num_classes))), batch_size=1, shuffle=True)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    train_loss, train_acc = epoch('train', synthetic_loader, model_synthetic, optimizer_synthetic, torch.nn.CrossEntropyLoss(), args, aug=False)\n",
    "    print(f\"[Synthetic Data] Epoch {epoch_num + 1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate model trained on synthetic data\n",
    "accuracy_synthetic = evaluate_model(model_synthetic, test_loader, device)\n",
    "print(f\"Test Accuracy of model trained on synthetic data: {accuracy_synthetic:.2f}%\")\n",
    "\n",
    "# PART 2: Train on Real Data\n",
    "print(\"Training on Real Data...\")\n",
    "model_real = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "optimizer_real = optim.SGD(model_real.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    train_loss, train_acc = epoch('train', real_data_loader, model_real, optimizer_real, torch.nn.CrossEntropyLoss(), args, aug=True)\n",
    "    print(f\"[Real Data] Epoch {epoch_num + 1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate model trained on real data\n",
    "accuracy_real = evaluate_model(model_real, test_loader, device)\n",
    "print(f\"Test Accuracy of model trained on real data: {accuracy_real:.2f}%\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(f\"Model trained on synthetic data - Test Accuracy: {accuracy_synthetic:.2f}%\")\n",
    "print(f\"Model trained on real data - Test Accuracy: {accuracy_real:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0968f-f1a1-45ce-bb55-519cafa8f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 3\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from utils import get_network, evaluate_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# List of model architectures to evaluate\n",
    "model_architectures = ['ConvNetD3', 'AlexNet', 'ResNet18']  # Replace or expand with desired architectures\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DataLoader for synthetic data\n",
    "synthetic_loader = DataLoader(list(zip(synthetic_data, torch.arange(num_classes))), batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialize MHIST test loader for evaluation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training and evaluation function\n",
    "def train_on_synthetic(model, synthetic_loader, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train the model on synthetic data.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in synthetic_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluate each model architecture\n",
    "results = {}\n",
    "for arch in model_architectures:\n",
    "    # Initialize and train the model\n",
    "    model = get_network(arch, channel, num_classes, im_size).to(device)\n",
    "    train_on_synthetic(model, synthetic_loader)\n",
    "\n",
    "    # Evaluate on MHIST test set\n",
    "    accuracy = evaluate_model(model, test_loader, device)\n",
    "    results[arch] = accuracy\n",
    "    print(f\"{arch} - Test Accuracy on MHIST: {accuracy:.2f}%\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Model Performance on Synthetic Dataset:\")\n",
    "for arch, accuracy in results.items():\n",
    "    print(f\"{arch} - Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470535a6-160a-4a1d-ab29-2b9fecb428d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 4\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from utils import match_loss, get_network, evaluate_model\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters for ablation\n",
    "ipc_values = [1, 5, 10]  # Different numbers of images per class\n",
    "T_values = [5, 10, 20]   # Different numbers of SAM iterations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Results DataFrame to store outcomes\n",
    "results_df = pd.DataFrame(columns=['IPC', 'T', 'Model', 'Test Accuracy'])\n",
    "\n",
    "# Function to perform SAM with specific ipc and T\n",
    "def perform_sam(ipc, T, model_name='ConvNetD3'):\n",
    "    # Initialize synthetic data based on ipc\n",
    "    synthetic_data = torch.randn(num_classes, channel, im_size[0], im_size[1], requires_grad=True, device=device) * ipc\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = get_network(model_name, channel, num_classes, im_size).to(device)\n",
    "    optimizer_syn = optim.SGD([synthetic_data], lr=0.1)\n",
    "    \n",
    "    # SAM training loop\n",
    "    for _ in range(100):  # Fixed number of outer loops\n",
    "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        optimizer_model = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "        for _ in range(T):\n",
    "            optimizer_syn.zero_grad()\n",
    "            optimizer_model.zero_grad()\n",
    "\n",
    "            # Fetch a batch of real data\n",
    "            real_images, real_labels = next(iter(real_data_loader))\n",
    "            real_images, real_labels = real_images.to(device), real_labels.to(device)\n",
    "\n",
    "            # Subset real data to match synthetic data size\n",
    "            indices = torch.randperm(real_images.size(0))[:ipc]\n",
    "            real_images_subset = real_images[indices]\n",
    "            real_labels_subset = real_labels[indices]\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs_real = model(real_images_subset)\n",
    "            outputs_syn = model(synthetic_data)\n",
    "            loss = match_loss(outputs_syn, outputs_real, args=None) + 0.01  # Lambda parameter\n",
    "            loss.backward()\n",
    "            optimizer_syn.step()\n",
    "\n",
    "    return synthetic_data, model\n",
    "\n",
    "# Run ablation study\n",
    "for ipc in ipc_values:\n",
    "    for T in T_values:\n",
    "        # Perform SAM and train the model\n",
    "        synthetic_data, model = perform_sam(ipc, T, model_name='ConvNetD3')\n",
    "        \n",
    "        # Evaluate model on MHIST test set\n",
    "        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "        accuracy = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        # Log results\n",
    "        results_df = results_df.append({'IPC': ipc, 'T': T, 'Model': 'ConvNetD3', 'Test Accuracy': accuracy}, ignore_index=True)\n",
    "        print(f\"IPC: {ipc}, T: {T}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAblation Study Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
